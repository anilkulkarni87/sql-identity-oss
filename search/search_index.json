{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SQL Identity Resolution - Clean Docs","text":"<p>Welcome. This folder is a clean, code-aligned documentation set for the current CLI-first IDR system.</p> <p>Start here: - Getting started: 03-quickstart.md - Configure your data: 04-configuration.md - Run with the CLI: 05-cli.md - Platform setup: 06-platforms.md</p> <p>Table of contents: - 01-overview.md - 02-installation.md - 03-quickstart.md - 04-configuration.md - 05-cli.md - 06-platforms.md - 07-pipeline.md - 08-schema.md - 09-ui.md - 10-api.md - 11-mcp.md - 12-dbt.md - 13. Scale Testing - 14. Troubleshooting - 15. CI/CD - 16. Security - 17. End-to-End Testing - 18. Architecture - 19. Matching Algorithm - 20. Data Model - 21. How It Works - 22. Metrics - 23. Benchmarks - 24. Cluster Sizing - 25. Scaling Considerations</p>"},{"location":"01-overview/","title":"Overview","text":"<p>SQL Identity Resolution (IDR) is a warehouse-native identity graph engine. It links records across sources using deterministic rules, and optionally refines clusters using fuzzy matching.</p> <p>Core ideas: - Deterministic matching builds an identity graph using exact identifiers (email, phone, loyalty_id, etc.). - Label propagation resolves connected components into clusters. - Optional fuzzy rules merge clusters using blocking + scoring. - Output tables provide resolved membership, cluster metrics, and golden profiles.</p> <p>Primary interfaces: - CLI: <code>idr</code> (recommended for production) - UI: browser-based setup wizard and explorer - API: FastAPI backend used by the UI - MCP server: agent access to results - dbt package: deterministic-only SQL implementation</p> <p>Typical flow: 1. Initialize schemas and tables. 2. Apply a YAML configuration (sources, rules, survivorship). 3. Run the pipeline (FULL or INCR). 4. Explore results via SQL, UI, API, or MCP.</p>"},{"location":"02-installation/","title":"Installation","text":"<p>Use pip extras to install the components you need.</p> <p>Quick path selection: - <code>pip</code> install: best for CLI and library usage. - <code>docker compose up --build</code>: best for local UI + API development. - <code>docker compose -f docker-compose.prod.yml --env-file .env up -d</code>: best for reproducible deployments from pinned images. - <code>docker compose -f docker-compose.enterprise.yml up -d --build</code>: best for SSO/observability validation.</p> <p>Base + DuckDB (local/dev): <pre><code>pip install \"sql-identity-resolution[duckdb]\"\n</code></pre></p> <p>API server: <pre><code>pip install \"sql-identity-resolution[api]\"\n</code></pre></p> <p>Standalone UI + API (Docker): <pre><code>docker compose up --build\n</code></pre></p> <p>Pinned production images (reproducible): <pre><code>cp .env.example .env\ndocker compose -f docker-compose.prod.yml --env-file .env up -d\n</code></pre></p> <p>MCP server: <pre><code>pip install \"sql-identity-resolution[mcp]\"\n</code></pre></p> <p>All platforms: <pre><code>pip install \"sql-identity-resolution[all]\"\n</code></pre></p> <p>Verify: <pre><code>idr version\n</code></pre></p> <p>See <code>27-distribution.md</code> for compatibility matrix and deployment decision tree.</p> <p>CI and release automation use pinned lockfiles under <code>requirements/*.lock</code>.</p>"},{"location":"03-quickstart/","title":"Quickstart (DuckDB)","text":"<p>Run a full demo locally in minutes.</p> <pre><code>pip install \"sql-identity-resolution[duckdb]\"\nidr quickstart\n</code></pre> <p>Optional flags: <pre><code>idr quickstart --rows=50000 --output=demo.duckdb --seed=42\n</code></pre></p> <p>What it does: - Generates synthetic retail data - Initializes schemas and metadata - Runs the identity pipeline - Prints a run summary</p> <p>Next steps: - Inspect outputs with SQL (see 08-schema.md) - Apply your own config (see 04-configuration.md)</p>"},{"location":"04-configuration/","title":"Configuration Guide","text":"<p>Configuration is managed via YAML files and the <code>idr config</code> command. Source of Truth: <code>idr_core/config.py</code> definitions.</p>"},{"location":"04-configuration/#structure","title":"Structure","text":"<pre><code>sources:\n  - id: \"users\"\n    table: \"raw.users\"\n    entity_key: \"user_id\"\n    entity_type: \"PERSON\"\n    watermark_column: \"updated_at\"\n    identifiers:\n      - type: \"email\"\n        expr: \"LOWER(email)\"\n    attributes:\n      - name: \"first_name\"\n        expr: \"first_name\"\n\nrules:\n  - id: \"email_exact\"\n    type: \"EXACT\"\n    match_keys: [\"email\"]\n    priority: 10\n\nfuzzy_rules:\n  - id: \"name_fuzzy\"\n    blocking_key: \"metaphone(last_name)\"\n    score_expr: \"jaro_winkler(a.name, b.name)\"\n    threshold: 0.9\n\nsurvivorship:\n  - attribute: \"first_name\"\n    strategy: \"RECENCY\"\n    recency_field: \"updated_at\"\n</code></pre>"},{"location":"04-configuration/#sections","title":"Sections","text":""},{"location":"04-configuration/#sources-required","title":"<code>sources</code> (Required)","text":"<p>Defines input tables. *   <code>id</code>: Unique identifier for the source (used in lineage). *   <code>table</code>: Fully qualified table name (<code>schema.table</code>). *   <code>entity_key</code>: Unique primary key of the source entity. *   <code>identifiers</code>: List of identity columns to extract. *   <code>attributes</code>: List of profile attributes to extract.</p>"},{"location":"04-configuration/#rules","title":"<code>rules</code>","text":"<p>Deterministic matching rules. *   <code>type</code>: Currently only <code>EXACT</code> is supported. *   <code>match_keys</code>: List of identifier types to match on (e.g., <code>[\"email\"]</code>). *   <code>priority</code>: Lower number = higher priority edge.</p>"},{"location":"04-configuration/#survivorship","title":"<code>survivorship</code>","text":"<p>Golden profile resolution logic. *   <code>strategy</code>:     *   <code>RECENCY</code>: Latest value wins.     *   <code>PRIORITY</code>: Source with highest trust rank wins.     *   <code>FREQUENCY</code>: Most common value wins.     *   <code>AGG_MAX</code>, <code>AGG_SUM</code>: Aggregation functions.</p>"},{"location":"04-configuration/#fuzzy_rules","title":"<code>fuzzy_rules</code>","text":"<p>Probabilistic matching (requires fuzzy mode). *   <code>blocking_key</code>: SQL expression to limit comparison scope. *   <code>score_expr</code>: SQL expression returning float 0.0-1.0. *   <code>threshold</code>: Cutoff score for a match.</p>"},{"location":"05-cli/","title":"CLI Reference","text":"<p>The <code>idr</code> command-line interface is the primary way to interact with SQL Identity Resolution. Source of Truth: <code>idr_core/cli.py</code>.</p>"},{"location":"05-cli/#global-options","title":"Global Options","text":"Option Description <code>--help</code> Show help message and exit"},{"location":"05-cli/#commands","title":"Commands","text":""},{"location":"05-cli/#idr-run","title":"<code>idr run</code>","text":"<p>Execute the identity resolution pipeline.</p> <p>Usage: <pre><code>idr run --platform [PLATFORM] [CONNECTION_ARGS] [OPTIONS]\n</code></pre></p> <p>Options: | Option | Default | Description | |--------|---------|-------------| | <code>--platform</code> | Required | Target platform (<code>duckdb</code>, <code>bigquery</code>, <code>snowflake</code>, <code>databricks</code>) | | <code>--mode</code> | <code>FULL</code> | <code>FULL</code> (rebuild) or <code>INCR</code> (process updates) | | <code>--config</code> | None | YAML config file to apply before running | | <code>--max-iters</code> | <code>30</code> | Max label propagation iterations | | <code>--dry-run</code> | <code>False</code> | Preview changes without committing | | <code>--strict</code> | <code>False</code> | Disable fuzzy matching (deterministic only) |</p> <p>Connection Arguments: *   DuckDB: <code>--db PATH</code> *   BigQuery: <code>--project ID</code>, <code>--dataset</code> (default <code>idr_out</code>), <code>--meta-dataset</code> (<code>idr_meta</code>), <code>--work-dataset</code> (<code>idr_work</code>), <code>--location</code> (<code>US</code>) *   Snowflake: Uses env vars or <code>~/.snowflake/connections.toml</code> *   Databricks: Uses env vars <code>DATABRICKS_HOST</code>, <code>DATABRICKS_TOKEN</code>, <code>DATABRICKS_HTTP_PATH</code></p>"},{"location":"05-cli/#idr-init","title":"<code>idr init</code>","text":"<p>Initialize metadata tables (<code>idr_meta</code> schema).</p> <p>Usage: <pre><code>idr init --platform [PLATFORM] [--reset]\n</code></pre></p> <p>Options: *   <code>--reset</code>: Drop and recreate existing metadata tables (Destructive).</p>"},{"location":"05-cli/#idr-config","title":"<code>idr config</code>","text":"<p>Manage configuration files.</p> <ul> <li><code>validate --file config.yaml</code>: Check YAML structure against schema.</li> <li><code>generate --file config.yaml --dialect [duckdb|...]</code>: Output SQL statements.</li> <li><code>apply --file config.yaml --platform [PLATFORM]</code>: Execute SQL statements directly to DB.</li> </ul>"},{"location":"05-cli/#idr-quickstart","title":"<code>idr quickstart</code>","text":"<p>Run a zero-config demo on local DuckDB.</p> <p>Options: | Option | Default | Description | |--------|---------|-------------| | <code>--rows</code> | <code>10000</code> | Number of synthetic records | | <code>--output</code> | <code>quickstart_demo.duckdb</code> | Output file path | | <code>--seed</code> | <code>42</code> | Random seed |</p>"},{"location":"05-cli/#idr-mcp","title":"<code>idr mcp</code>","text":"<p>Start the Model Context Protocol (MCP) server.</p> <p>Options: *   <code>--transport</code>: <code>stdio</code> (default) or <code>sse</code> (requires uvicorn).</p>"},{"location":"05-cli/#idr-serve","title":"<code>idr serve</code>","text":"<p>Start the API server.</p> <p>Options: *   <code>--host</code>: Bind host (default <code>0.0.0.0</code>) *   <code>--port</code>: Bind port (default <code>8000</code>) *   <code>--reload</code>: Enable auto-reload (dev mode)</p> <p>Use <code>docker compose up</code> to run the standalone web UI with API.</p>"},{"location":"05-cli/#deprecated-runners","title":"Deprecated Runners","text":"<p>Warning: Direct usage of <code>python sql/*/idr_run.py</code> scripts is deprecated. Please use the unified <code>idr</code> CLI.</p>"},{"location":"06-platforms/","title":"Platforms and Connectivity","text":"<p>Detailed deployment guides for supported platforms.</p>"},{"location":"06-platforms/#duckdb-local-single-node","title":"DuckDB (Local / Single Node)","text":"<p>Ideal for local development, testing, and small-scale production (&lt; 10M rows).</p>"},{"location":"06-platforms/#setup","title":"Setup","text":"<ol> <li>Install: <code>pip install duckdb</code></li> <li>Initialize:     <pre><code>idr config apply --platform duckdb --db production.duckdb --file config.yaml\n</code></pre></li> <li>Run:     <pre><code>idr run --platform duckdb --db production.duckdb --mode FULL\n</code></pre></li> </ol>"},{"location":"06-platforms/#production-advice","title":"Production Advice","text":"<ul> <li>Use a persistent volume for the <code>.duckdb</code> file.</li> <li>Allocate sufficient RAM (2x dataset size recommended).</li> </ul>"},{"location":"06-platforms/#snowflake","title":"Snowflake","text":"<p>Scalable cloud data warehouse.</p>"},{"location":"06-platforms/#prerequisites","title":"Prerequisites","text":"<ul> <li>Account with <code>CREATE DATABASE</code> and <code>CREATE SCHEMA</code> privileges.</li> <li>Python environment with <code>snowflake-connector-python</code>.</li> </ul>"},{"location":"06-platforms/#setup_1","title":"Setup","text":"<ol> <li>Environment Variables:     <pre><code>export SNOWFLAKE_ACCOUNT=...\nexport SNOWFLAKE_USER=...\nexport SNOWFLAKE_PASSWORD=...\nexport SNOWFLAKE_WAREHOUSE=...\nexport SNOWFLAKE_DATABASE=...\n</code></pre></li> <li>Initialize:     <pre><code>idr config apply --platform snowflake --file config.yaml\n</code></pre></li> <li>Run:     <pre><code>idr run --platform snowflake --mode FULL\n</code></pre></li> </ol>"},{"location":"06-platforms/#production-advice_1","title":"Production Advice","text":"<ul> <li>Clustering: <code>ALTER TABLE idr_out.identity_edges_current CLUSTER BY (identifier_type)</code>.</li> <li>Performance: Use larger warehouses (L/XL) for initial &gt;100M loads.</li> </ul>"},{"location":"06-platforms/#bigquery","title":"BigQuery","text":"<p>Serverless, auto-scaling.</p>"},{"location":"06-platforms/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>GCP Project with BigQuery API enabled.</li> <li>Service Account with <code>BigQuery Job User</code> and <code>Data Editor</code>.</li> </ul>"},{"location":"06-platforms/#setup_2","title":"Setup","text":"<ol> <li>Auth: <code>export GOOGLE_APPLICATION_CREDENTIALS=key.json</code></li> <li>Initialize:     <pre><code>idr config apply --platform bigquery --project my-project --file config.yaml\n</code></pre></li> <li>Run:     <pre><code>idr run --platform bigquery --project my-project --mode FULL\n</code></pre></li> </ol>"},{"location":"06-platforms/#production-advice_2","title":"Production Advice","text":"<ul> <li>Fuzzy Matching: Uses native SQL functions (Levenshtein) optimized for BigQuery.</li> <li>Costs: Use slot reservations for predictable pricing on massive workloads.</li> </ul>"},{"location":"06-platforms/#databricks","title":"Databricks","text":"<p>Spark-based distributed processing.</p>"},{"location":"06-platforms/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Databricks Workspace (AWS/Azure/GCP).</li> <li>SQL Warehouse or All-Purpose Cluster.</li> <li>Unity Catalog (Recommended).</li> </ul>"},{"location":"06-platforms/#setup_3","title":"Setup","text":"<ol> <li>Env Vars:     <pre><code>export DATABRICKS_HOST=...\nexport DATABRICKS_TOKEN=...\nexport DATABRICKS_HTTP_PATH=...\n</code></pre></li> <li>Initialize:     <pre><code>export DATABRICKS_CATALOG=hive_metastore  # optional\nidr config apply --platform databricks --file config.yaml\n</code></pre></li> <li>Run:     <pre><code>idr run --platform databricks --mode FULL\n</code></pre></li> </ol>"},{"location":"06-platforms/#production-advice_3","title":"Production Advice","text":"<ul> <li>Photon: Enable Photon engine for 2-3x faster localized processing.</li> <li>Optimization: <code>OPTIMIZE</code> tables regularly to compact delta files.</li> </ul>"},{"location":"07-pipeline/","title":"Pipeline Deep Dive","text":"<p>The runner orchestrates four stages: preflight, extraction, graph, and output.</p>"},{"location":"07-pipeline/#1-preflight","title":"1. Preflight","text":"<ul> <li>Checks for concurrent runs.</li> <li>Verifies source tables exist.</li> <li>Applies schema upgrades (best-effort).</li> <li>Validates identifier mappings against actual columns.</li> </ul>"},{"location":"07-pipeline/#2-extraction","title":"2. Extraction","text":"<ul> <li>Builds <code>idr_work.entities_delta</code> based on watermarks (FULL or INCR).</li> <li>Extracts identifiers into <code>idr_work.identifiers_all</code>.</li> <li>Extracts attributes into <code>idr_work.entity_attributes</code> for fuzzy and survivorship.</li> <li>Applies exclusions from <code>idr_meta.identifier_exclusion</code> if present.</li> </ul>"},{"location":"07-pipeline/#3-graph","title":"3. Graph","text":"<ul> <li>Builds edges with an anchor-based N-1 approach (avoids O(N^2)).</li> <li>Skips identifier groups larger than <code>max_group_size</code>.</li> <li>Label propagation resolves connected components into clusters.</li> <li>Optional fuzzy matching builds super-clusters (skipped in <code>--strict</code>).</li> </ul>"},{"location":"07-pipeline/#4-output","title":"4. Output","text":"<ul> <li>Upserts membership, edges, and clusters.</li> <li>Computes confidence scores (edge diversity + match density).</li> <li>Updates watermarks in <code>idr_meta.run_state</code>.</li> <li>Builds golden profiles via <code>ProfileBuilder</code>.</li> </ul>"},{"location":"07-pipeline/#dry-run-mode","title":"Dry run mode","text":"<ul> <li>Runs all stages but writes to <code>idr_out.dry_run_results</code> and <code>idr_out.dry_run_summary</code>.</li> <li>Does not modify production tables.</li> </ul>"},{"location":"08-schema/","title":"Schema Reference","text":"<p>This reference documents the database tables managed by the <code>idr</code> CLI. Source of Truth: <code>sql/ddl/*.sql</code> files are the definitive source.</p>"},{"location":"08-schema/#idr_meta-configuration","title":"idr_meta (Configuration)","text":"<p>Tables in this schema control the behavior of the identity resolution pipeline.</p>"},{"location":"08-schema/#source_table","title":"source_table","text":"<p>Registry of source tables to process.</p> Column Type Description <code>table_id</code> STRING (PK) Unique identifier for source table <code>table_fqn</code> STRING Fully qualified name (schema.table) <code>entity_type</code> STRING Entity type (e.g., PERSON) <code>entity_key_expr</code> STRING SQL to generate unique entity key <code>watermark_column</code> STRING Column for incremental processing <code>watermark_lookback_minutes</code> INT Buffer time for late arriving data <code>is_active</code> BOOL Whether to include this source"},{"location":"08-schema/#rule","title":"rule","text":"<p>Deterministic matching rules.</p> Column Type Description <code>rule_id</code> STRING (PK) Unique rule identifier <code>rule_name</code> STRING Descriptive name <code>identifier_type</code> STRING Type of identifier to match <code>canonicalize</code> STRING Normalization (LOWERCASE, EXACT) <code>allow_hashed</code> BOOL Allow pre-hashed values <code>require_non_null</code> BOOL Skip if NULL <code>max_group_size</code> INT Max entities per group (hub breaking) <code>priority</code> INT Processing priority <code>is_active</code> BOOL Enabled status"},{"location":"08-schema/#fuzzy_rule","title":"fuzzy_rule","text":"<p>Probabilistic matching rules (used when <code>--strict</code> is not set).</p> Column Type Description <code>rule_id</code> STRING (PK) Unique rule identifier <code>rule_name</code> STRING Descriptive name <code>blocking_key_expr</code> STRING Pre-filter expression <code>score_expr</code> STRING Similarity function call <code>threshold</code> DOUBLE Match threshold (0.0-1.0) <code>priority</code> INT Processing priority <code>is_active</code> BOOL Enabled status"},{"location":"08-schema/#identifier_mapping","title":"identifier_mapping","text":"<p>Maps source columns to standardized identifier types.</p> Column Type Description <code>table_id</code> STRING (PK) FK to source_table <code>identifier_type</code> STRING (PK) Identifier type (email, phone) <code>identifier_value_expr</code> STRING SQL expression to extract value <code>is_hashed</code> BOOL If value is already hashed"},{"location":"08-schema/#entity_attribute_mapping","title":"entity_attribute_mapping","text":"<p>Maps source columns to golden profile attributes.</p> Column Type Description <code>table_id</code> STRING (PK) FK to source_table <code>attribute_name</code> STRING (PK) Normalized attribute name <code>attribute_expr</code> STRING SQL expression for value"},{"location":"08-schema/#survivorship_rule","title":"survivorship_rule","text":"<p>Rules for resolving Golden Profile conflicts.</p> Column Type Description <code>attribute_name</code> STRING (PK) Attribute to resolve <code>strategy</code> STRING RECENCY, PRIORITY, FREQUENCY <code>source_priority_list</code> STRING Prioritized list of table_ids <code>recency_field</code> STRING Field for RECENCY strategy <code>is_active</code> BOOL Enabled status"},{"location":"08-schema/#identifier_exclusion","title":"identifier_exclusion","text":"<p>Blocklist for specific identifier values (e.g., 'null', 'test').</p> Column Type Description <code>identifier_type</code> STRING Type of identifier <code>identifier_value_pattern</code> STRING Value or LIKE pattern <code>match_type</code> STRING 'EXACT' or 'LIKE' <code>reason</code> STRING Audit reason <code>created_at</code> TIMESTAMP Creation time <code>created_by</code> STRING Creator"},{"location":"08-schema/#run_state","title":"run_state","text":"<p>Internal state tracking for incremental processing.</p> Column Type Description <code>table_id</code> STRING (PK) Source table ID <code>last_watermark_value</code> TIMESTAMP Last processed timestamp <code>last_run_id</code> STRING Last successful run ID <code>last_run_ts</code> TIMESTAMP Last run timestamp"},{"location":"08-schema/#config","title":"config","text":"<p>Global system configuration.</p> Column Type Description <code>config_key</code> STRING (PK) Configuration setting name <code>config_value</code> STRING Setting value <code>description</code> STRING Setting description <code>updated_at</code> TIMESTAMP Last update time <code>updated_by</code> STRING User who updated"},{"location":"08-schema/#idr_out-output","title":"idr_out (Output)","text":"<p>Results of the resolution process.</p>"},{"location":"08-schema/#identity_resolved_membership_current","title":"identity_resolved_membership_current","text":"<p>Maps entities to Resolved Identity Clusters.</p> Column Type Description <code>entity_key</code> STRING (PK) Unique source entity key <code>resolved_id</code> STRING The Identity Cluster ID <code>run_id</code> STRING Run responsible for last update <code>super_cluster_id</code> STRING Optional higher-level grouping <code>updated_ts</code> TIMESTAMP Last update time"},{"location":"08-schema/#identity_clusters_current","title":"identity_clusters_current","text":"<p>Master list of Identity Clusters.</p> Column Type Description <code>resolved_id</code> STRING (PK) Identity Cluster ID <code>cluster_size</code> BIGINT Count of entities <code>confidence_score</code> DOUBLE Quality score (0.0-1.0) <code>primary_reason</code> STRING Reason for score <code>run_id</code> STRING Last update run <code>updated_ts</code> TIMESTAMP Last update time"},{"location":"08-schema/#identity_edges_current","title":"identity_edges_current","text":"<p>Graph edges between entities.</p> Column Type Description <code>left_entity_key</code> STRING (PK) Entity A <code>right_entity_key</code> STRING (PK) Entity B <code>identifier_type</code> STRING (PK) Linking identifier type <code>identifier_value_norm</code> STRING Normalized value <code>rule_id</code> STRING Rule responsible for edge <code>first_seen_ts</code> TIMESTAMP Creation time <code>last_seen_ts</code> TIMESTAMP Last verified time <code>run_id</code> STRING Last update run"},{"location":"08-schema/#run_history","title":"run_history","text":"<p>Audit log of executed runs.</p> Column Type Description <code>run_id</code> STRING (PK) Unique Run ID <code>run_mode</code> STRING FULL or INCR <code>status</code> STRING SUCCESS, FAILED <code>entities_processed</code> BIGINT Count <code>edges_created</code> BIGINT Count <code>duration_seconds</code> BIGINT Execution time <code>error_message</code> STRING Error details (if failed) <code>watermarks_json</code> STRING JSON snapshot of watermarks"},{"location":"09-ui/","title":"Web UI","text":"<p>The web UI provides: - Setup Wizard for initial configuration - Dashboard for run metrics - Explorer for cluster investigation</p> <p>Run API only (no standalone UI bundle): <pre><code>idr serve\n</code></pre> Open API docs at http://localhost:8000/docs.</p> <p>Run full API + UI stack: <pre><code>docker compose up --build\n</code></pre> Open UI at http://localhost:3000.</p>"},{"location":"09-ui/#setup-wizard","title":"Setup Wizard","text":"<p>Steps: 1. Connect: choose platform and provide credentials 2. Discover: list tables and columns 3. Map: map identifiers and attributes 4. Rules: configure deterministic + fuzzy rules 5. Survivorship: configure golden record strategies 6. Review: save config and run</p>"},{"location":"09-ui/#explorer","title":"Explorer","text":"<ul> <li>Search by email/phone/ID</li> <li>View cluster graph and edges</li> </ul>"},{"location":"09-ui/#dashboard","title":"Dashboard","text":"<ul> <li>Run summary and metrics</li> <li>Confidence scores</li> <li>Recent runs</li> </ul>"},{"location":"10-api/","title":"API Reference (Summary)","text":"<p>The API is a FastAPI service used by the UI. It also supports direct use.</p> <p>Base URL: http://localhost:8000</p>"},{"location":"10-api/#health","title":"Health","text":"<ul> <li><code>GET /api/health</code></li> </ul>"},{"location":"10-api/#connection","title":"Connection","text":"<ul> <li><code>POST /api/connect</code> (connect for dashboard/explorer)</li> </ul>"},{"location":"10-api/#setup-wizard","title":"Setup Wizard","text":"<ul> <li><code>POST /api/setup/connect</code></li> <li><code>GET /api/setup/status</code></li> <li><code>GET /api/setup/config</code></li> <li><code>GET /api/setup/discover/tables?schema=...</code></li> <li><code>GET /api/setup/discover/columns?table=...</code></li> <li><code>POST /api/setup/config/save</code></li> <li><code>POST /api/setup/run</code></li> <li><code>GET /api/setup/fuzzy-templates</code></li> </ul>"},{"location":"10-api/#metrics","title":"Metrics","text":"<ul> <li><code>GET /api/metrics/summary</code></li> <li><code>GET /api/metrics/distribution</code></li> <li><code>GET /api/metrics/rules</code></li> <li><code>GET /api/alerts</code></li> </ul>"},{"location":"10-api/#explorer","title":"Explorer","text":"<ul> <li><code>GET /api/entities/search?q=...</code></li> <li><code>GET /api/clusters/{resolved_id}</code></li> </ul>"},{"location":"10-api/#runs","title":"Runs","text":"<ul> <li><code>GET /api/runs</code></li> </ul>"},{"location":"10-api/#schema","title":"Schema","text":"<ul> <li><code>GET /api/schema</code></li> </ul> <p>Notes: - The API uses the current adapter from the ConnectionManager. - Setup endpoints are intended for the wizard and can initialize metadata tables.</p>"},{"location":"11-mcp/","title":"MCP Server","text":"<p>The MCP server exposes read-only tools for agents (clusters, profiles, edges, run history).</p> <p>Start: <pre><code>export IDR_PLATFORM=duckdb\nexport IDR_DATABASE=./demo.duckdb\nidr mcp\n</code></pre></p> <p>PII masking: - Default is masked - Set <code>IDR_PII_ACCESS=full</code> for unmasked values</p> <p>Key tools: - <code>get_cluster(resolved_id, include_edges, include_entities)</code> - <code>get_golden_profile(resolved_id)</code> - <code>search_identifier(value, identifier_type, limit)</code> - <code>list_edges_for_cluster(resolved_id)</code> - <code>explain_edge(entity_key_a, entity_key_b)</code> - <code>run_history(limit)</code> - <code>latest_run()</code> - <code>config_snapshot(config_hash)</code> - <code>list_rules()</code> - <code>list_sources()</code></p> <p>Connection is established from environment variables on startup.</p>"},{"location":"12-dbt/","title":"dbt Package","text":"<p>The dbt package provides deterministic-only identity resolution using SQL models.</p> <p>Install: <pre><code>packages:\n  - git: \"https://github.com/anilkulkarni87/sql-identity-resolution\"\n    subdirectory: \"dbt_idr\"\n    revision: main\n</code></pre></p> <p>Seed configuration: - <code>idr_sources.csv</code> - <code>idr_rules.csv</code> - <code>idr_identifier_mappings.csv</code> - <code>idr_attribute_mappings.csv</code> - <code>idr_survivorship_rules.csv</code> - <code>idr_exclusions.csv</code></p> <p>Run: <pre><code>dbt deps\ndbt seed --select dbt_idr\ndbt run --select dbt_idr\n</code></pre></p> <p>Notes: - Fuzzy matching is not supported in dbt. - Golden profiles support RECENCY and PRIORITY.</p>"},{"location":"13-scale-testing/","title":"Scale Testing","text":"<p>Validate performance with large synthetic datasets.</p>"},{"location":"13-scale-testing/#data-generation","title":"Data Generation","text":"<p>Use the <code>generate_global_retail_idr.py</code> script to create realistic test data at scale.</p> <pre><code># Generate 1 Million rows\npython tools/scale_test/generate_global_retail_idr.py \\\n  --rows=1000000 \\\n  --output=data/1m_test\n</code></pre>"},{"location":"13-scale-testing/#running-the-benchmark","title":"Running the Benchmark","text":"<ol> <li> <p>Load Data: Use the platform-specific loader.     <pre><code>python tools/scale_test/load_duckdb.py --db=scale.duckdb --input=data/1m_test\n</code></pre></p> </li> <li> <p>Initialize:     <pre><code>idr init --platform duckdb --db scale.duckdb --reset\n</code></pre></p> </li> <li> <p>Run IDR:     <pre><code>idr run --platform duckdb --db scale.duckdb --mode FULL\n</code></pre></p> </li> </ol>"},{"location":"13-scale-testing/#sizing-reference","title":"Sizing Reference","text":"<p>See <code>docs/cluster_sizing.md</code> (to be migrated) for expected resource usage.</p>"},{"location":"14-troubleshooting/","title":"Troubleshooting","text":""},{"location":"14-troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"14-troubleshooting/#1-table-not-found-errors","title":"1. \"Table not found\" errors","text":"<p>Cause: Metadata tables not initialized. Fix: Run <code>idr init</code> for your platform.</p>"},{"location":"14-troubleshooting/#2-giant-cluster-warnings","title":"2. \"Giant Cluster\" warnings","text":"<p>Cause: A common value (e.g., \"null\", \"n/a\") is linking thousands of entities. Fix: Add the value to <code>idr_meta.identifier_exclusion</code>. <pre><code>INSERT INTO idr_meta.identifier_exclusion (identifier_type, identifier_value_pattern, match_type, reason)\nVALUES ('email', 'no-reply@%', 'LIKE', 'Generic email');\n</code></pre></p>"},{"location":"14-troubleshooting/#3-pipeline-failing-at-label-propagation","title":"3. Pipeline failing at \"Label Propagation\"","text":"<p>Cause: Graph explosion due to high connectivity. Fix: *   Check <code>skipped_identifier_groups</code> table for clues. *   Lower <code>max_group_size</code> in <code>idr_meta.rule</code>. *   Increase <code>max_lp_iterations</code> if it's just slow to converge.</p>"},{"location":"14-troubleshooting/#debugging","title":"Debugging","text":""},{"location":"14-troubleshooting/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<p>Set <code>LOG_LEVEL=DEBUG</code> in your environment.</p>"},{"location":"14-troubleshooting/#check-run-history","title":"Check Run History","text":"<pre><code>SELECT * FROM idr_out.run_history ORDER BY started_at DESC LIMIT 1;\n</code></pre>"},{"location":"14-troubleshooting/#dry-run","title":"Dry Run","text":"<p>Use <code>--dry-run</code> to see what changes would happen without committing them. <pre><code>idr run --platform bigquery --mode INCR --dry-run\n</code></pre></p>"},{"location":"15-ci-cd/","title":"CI/CD","text":"<p>Continuous integration and deployment pipelines for SQL Identity Resolution.</p>"},{"location":"15-ci-cd/#github-actions","title":"GitHub Actions","text":""},{"location":"15-ci-cd/#test-workflows","title":"Test Workflows","text":"<p>Automated testing on pull requests ensures stability across platforms.</p> <pre><code># .github/workflows/test.yml\nname: Test\non: [push, pull_request]\njobs:\n  test-duckdb:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with: {python-version: '3.11'}\n      - run: pip install duckdb pytest\n      - run: python -m pytest tests/ --ignore=tests/legacy -v\n</code></pre>"},{"location":"15-ci-cd/#package-smoke-gate","title":"Package Smoke Gate","text":"<p><code>test.yml</code> also builds wheel/sdist and installs from the built wheel before tests.</p> <pre><code>jobs:\n  package-smoke:\n    steps:\n      - run: python -m build\n      - run: twine check dist/*\n      - run: |\n          python -m venv .venv-smoke\n          . .venv-smoke/bin/activate\n          pip install dist/*.whl\n          idr version\n</code></pre>"},{"location":"15-ci-cd/#lockfile-enforcement","title":"Lockfile Enforcement","text":"<p>CI dependency installation uses pinned lockfiles: - <code>requirements/ci.lock</code> for test, lint, and DDL validation jobs - <code>requirements/release.lock</code> for package/release artifact jobs - <code>requirements/docs.lock</code> for docs workflow</p> <p>UI dependencies are verified with <code>npm ci</code> to enforce <code>idr_ui/package-lock.json</code>.</p>"},{"location":"15-ci-cd/#sbom-verification","title":"SBOM Verification","text":"<p><code>test.yml</code> includes an <code>sbom-verify</code> job that: 1. Builds Python artifacts (<code>dist/*</code>) 2. Builds API and UI Docker images 3. Generates SPDX JSON SBOMs for each artifact/image 4. Verifies SBOM structure in CI using <code>jq</code> checks 5. Uploads SBOMs as workflow artifacts</p>"},{"location":"15-ci-cd/#enterprise-e2e-gate","title":"Enterprise E2E Gate","text":"<p><code>test.yml</code> includes <code>enterprise-e2e</code> that validates the enterprise stack end-to-end: 1. Boots <code>docker-compose.enterprise.yml</code> 2. Verifies Keycloak realm bootstrap 3. Verifies API health and Prometheus health 4. Confirms <code>/metrics</code> payload 5. Confirms auth enforcement on protected endpoints 6. Retrieves OIDC token and validates authenticated API access 7. Verifies Prometheus target health for <code>idr-api</code></p>"},{"location":"15-ci-cd/#release-workflow","title":"Release Workflow","text":"<p><code>release.yml</code> is tag-driven (<code>v*</code>) and produces reproducible artifacts: - Python wheel + sdist - Wheel install smoke check - GHCR images for <code>idr-api</code> and <code>idr-ui</code> - Trivy vulnerability gating on dist artifacts and container images (fails on HIGH/CRITICAL) - Signed build provenance attestations for dist artifacts and pushed images</p> <p>Manual runs are available through <code>workflow_dispatch</code> to optionally push images or publish package. Release summary includes an attestation link: <code>https://github.com/&lt;owner&gt;/&lt;repo&gt;/attestations</code></p>"},{"location":"15-ci-cd/#ddl-validation","title":"DDL Validation","text":"<p>Ensure SQL schema definitions are valid before merging.</p> <pre><code># .github/workflows/validate-ddl.yml\nname: Validate DDL\non:\n  pull_request:\n    paths: ['sql/**/*.sql']\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install duckdb\n      - run: |\n          python -c \"\n          import duckdb\n          conn = duckdb.connect(':memory:')\n          with open('sql/ddl/duckdb.sql') as f:\n              conn.execute(f.read())\n          print('DuckDB DDL valid')\n          \"\n</code></pre>"},{"location":"15-ci-cd/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Use pre-commit to catch issues locally.</p> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n        language_version: python3.11\n\n  - repo: local\n    hooks:\n      - id: validate-ddl\n        name: Validate DuckDB DDL\n        entry: python -c \"import duckdb; duckdb.connect(':memory:').execute(open('sql/ddl/duckdb.sql').read())\"\n        language: python\n        files: sql/ddl/.*\\.sql$\n        additional_dependencies: [duckdb]\n</code></pre>"},{"location":"15-ci-cd/#secrets-management","title":"Secrets Management","text":"Secret Description <code>SNOWFLAKE_ACCOUNT</code> Snowflake Account ID <code>SNOWFLAKE_USER</code> Service Account User <code>GCP_PROJECT</code> BigQuery Project ID <code>DATABRICKS_HOST</code> Workspace URL <code>DATABRICKS_TOKEN</code> Access Token <p>Use GitHub Environments (<code>snowflake-test</code>, <code>gcp-test</code>) to isolate secrets.</p>"},{"location":"16-security/","title":"Security","text":"<p>Security best practices for deploying Identity Resolution.</p>"},{"location":"16-security/#principle-of-least-privilege","title":"Principle of Least Privilege","text":""},{"location":"16-security/#snowflake","title":"Snowflake","text":"<p>Create a dedicated <code>IDR_EXECUTOR</code> role.</p> <pre><code>CREATE ROLE IDR_EXECUTOR;\n-- Grant usage on warehouse and database\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE IDR_EXECUTOR;\nGRANT USAGE ON DATABASE analytics TO ROLE IDR_EXECUTOR;\n\n-- Read-only on source data\nGRANT SELECT ON ALL TABLES IN SCHEMA crm TO ROLE IDR_EXECUTOR;\n\n-- Full control of IDR schemas\nGRANT ALL ON SCHEMA idr_meta TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_work TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_out TO ROLE IDR_EXECUTOR;\n</code></pre>"},{"location":"16-security/#bigquery","title":"BigQuery","text":"<p>Use a dedicated Service Account with granular IAM roles. *   <code>roles/bigquery.jobUser</code> (Project level) *   <code>roles/bigquery.dataViewer</code> (Source datasets) *   <code>roles/bigquery.dataEditor</code> (IDR datasets)</p>"},{"location":"16-security/#data-protection","title":"Data Protection","text":""},{"location":"16-security/#pii-handling","title":"PII Handling","text":"<ul> <li>Encryption: Ensure encryption at rest and in transit (standard on cloud DWHs).</li> <li>Retention: Regularly clean up <code>idr_out.dry_run_results</code> and <code>idr_work</code> tables.</li> <li>Masking: Apply Dynamic Data Masking policies on <code>golden_profile_current</code> output if accessed by broad teams.</li> </ul>"},{"location":"16-security/#credential-management","title":"Credential Management","text":"<ul> <li>Never hardcode passwords in scripts or config files.</li> <li>Use environment variables (<code>SNOWFLAKE_PASSWORD</code>).</li> <li>In production, inject secrets via AWS Secrets Manager / GCP Secret Manager / Azure Key Vault.</li> </ul>"},{"location":"16-security/#network-security","title":"Network Security","text":"<ul> <li>Snowflake: Use Network Policies to whitelist IP ranges.</li> <li>BigQuery: Use VPC Service Controls.</li> <li>Databricks: Deploy in a private subnet with PrivateLink.</li> </ul>"},{"location":"16-security/#api-token-validation","title":"API Token Validation","text":"<p>The API validates bearer tokens against OIDC JWKS with: - <code>kid</code>-based signing key selection - Audience and issuer enforcement - JWKS cache with configurable TTL (<code>IDR_AUTH_JWKS_TTL_SECONDS</code>) - Configurable JWKS fetch timeout (<code>IDR_AUTH_JWKS_HTTP_TIMEOUT_SECONDS</code>)</p>"},{"location":"17-e2e-testing/","title":"End-to-End Testing","text":"<p>Validation guide for the full Identity Resolution pipeline.</p>"},{"location":"17-e2e-testing/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install numpy pyarrow faker\ncd tools/scale_test\n</code></pre>"},{"location":"17-e2e-testing/#1-generate-test-data","title":"1. Generate Test Data","text":"<p>Create a synthetic dataset with known ground truth. <pre><code>python generate_global_retail_idr.py --rows 100000 --output data/100k_test\n</code></pre></p>"},{"location":"17-e2e-testing/#2-load-and-initialize","title":"2. Load and Initialize","text":"<p>Example for DuckDB: <pre><code># Load data\npython load_duckdb.py --db test.db --input data/100k_test\n\n# Initialize IDR schemas\nidr init --platform duckdb --db test.db --reset\n\n# Apply metadata config\ncat setup_retail_metadata.sql | duckdb test.db\n</code></pre></p>"},{"location":"17-e2e-testing/#3-run-pipeline","title":"3. Run Pipeline","text":"<pre><code>idr run --platform duckdb --db test.db --mode FULL\n</code></pre>"},{"location":"17-e2e-testing/#4-validate-results","title":"4. Validate Results","text":"<p>Compare IDR output against the known ground truth in the generated data. <pre><code>python run_validation.py --platform duckdb --db test.db\n</code></pre></p>"},{"location":"17-e2e-testing/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<ul> <li>Precision: % of entities correctly clustered together.</li> <li>Recall: % of true matches found.</li> <li>Over-clustering: Are distinct people merged? (False Positives)</li> <li>Under-clustering: Are same people split? (False Negatives)</li> </ul>"},{"location":"17-e2e-testing/#5-clean-up","title":"5. Clean Up","text":"<pre><code>rm test.db\nrm -rf data/100k_test\n</code></pre>"},{"location":"18-architecture/","title":"Architecture","text":"<p>System architecture and data flow.</p>"},{"location":"18-architecture/#high-level-design","title":"High-Level Design","text":"<pre><code>graph TB\n    Sources[Source Systems] --&gt; Extract[Extract &amp; Normalize]\n    Extract --&gt; Edges[Edge Building]\n    Edges --&gt; LP[Label Propagation]\n    LP --&gt; Clusters[Cluster Assignment]\n    Clusters --&gt; Golden[Golden Profile]</code></pre>"},{"location":"18-architecture/#layers","title":"Layers","text":""},{"location":"18-architecture/#1-metadata-idr_meta","title":"1. Metadata (<code>idr_meta</code>)","text":"<ul> <li>Source Registry: Definitions of input tables.</li> <li>Rules: Exact and fuzzy matching logic.</li> <li>Mappings: Transformations for identifiers and attributes.</li> </ul>"},{"location":"18-architecture/#2-processing-idr_work","title":"2. Processing (<code>idr_work</code>)","text":"<ul> <li>Transient State: <code>identifiers</code>, <code>edges_new</code>, <code>lp_labels</code>.</li> <li>Incremental Delta: Only processes changed records.</li> </ul>"},{"location":"18-architecture/#3-output-idr_out","title":"3. Output (<code>idr_out</code>)","text":"<ul> <li>Identity Graph: <code>identity_resolved_membership_current</code>.</li> <li>Profiles: <code>golden_profile_current</code>.</li> <li>Audit: <code>run_history</code>.</li> </ul>"},{"location":"18-architecture/#cross-platform-execution","title":"Cross-Platform Execution","text":"<p>The core logic is SQL-based, allowing execution on: *   DuckDB: Local/Single-node. *   Snowflake: Cloud Data Warehouse. *   BigQuery: Serverless SQL. *   Databricks: Spark SQL (Photon).</p>"},{"location":"19-matching-algorithm/","title":"Matching Algorithm","text":"<p>The two-stage algorithm for entity resolution.</p>"},{"location":"19-matching-algorithm/#phase-1-deterministic-strict","title":"Phase 1: Deterministic (Strict)","text":""},{"location":"19-matching-algorithm/#1-extraction","title":"1. Extraction","text":"<p>Extracts identifiers (Email, Phone) from source records based on <code>identifier_mapping</code>. *   Normalization: Lowercase, trim, remove non-digits.</p>"},{"location":"19-matching-algorithm/#2-edge-building","title":"2. Edge Building","text":"<p>Connects entities sharing the same normalized identifier. *   Anchor Optimization: To avoid $O(N^2)$ connections in large groups, all members connect to a single \"Anchor\" (lowest <code>entity_key</code>).</p>"},{"location":"19-matching-algorithm/#3-label-propagation","title":"3. Label Propagation","text":"<p>Iteratively propagates the Cluster ID across the graph. *   Convergence: When no labels change, Connected Components are found.</p>"},{"location":"19-matching-algorithm/#phase-2-fuzzy-optional","title":"Phase 2: Fuzzy (Optional)","text":"<p>If <code>--strict=false</code>: 1.  Blocking: Groups candidates by a key (e.g., <code>Metaphone(Name)</code>). 2.  Scoring: Calculates similarity (e.g., Jaro-Winkler). 3.  Threshold: Merges clusters if Score &gt; Threshold.</p>"},{"location":"20-data-model/","title":"Data Model","text":"<p>Conceptual Entity-Relationship Diagram.</p>"},{"location":"20-data-model/#diagram","title":"Diagram","text":"<pre><code>erDiagram\n    SourceTable ||--o{ IdentifierMapping : defines\n    Rule ||--o{ IdentifierMapping : uses\n    Entity ||--o{ Identifier : has\n    Entity }o--|| Cluster : belongs_to\n    Cluster ||--|| GoldenProfile : has</code></pre>"},{"location":"20-data-model/#key-concepts","title":"Key Concepts","text":""},{"location":"20-data-model/#entities","title":"Entities","text":"<p>A distinct record from a source table (e.g., a row in <code>Salesforce.Contacts</code>). *   Key: <code>entity_key</code> (Source + PCK)</p>"},{"location":"20-data-model/#identifiers","title":"Identifiers","text":"<p>A piece of evidence used to link entities (e.g., <code>john@example.com</code>). *   Type: <code>EMAIL</code>, <code>PHONE</code>, <code>DEVICE_ID</code>.</p>"},{"location":"20-data-model/#clusters","title":"Clusters","text":"<p>A group of entities resolved to the same real-world person. *   Key: <code>resolved_id</code>.</p>"},{"location":"20-data-model/#golden-profile","title":"Golden Profile","text":"<p>The consolidated \"Best Version\" of the person, derived from all entities in the cluster using Survivorship Rules.</p>"},{"location":"21-how-it-works/","title":"How It Works","text":"<p>A simple explanation of the identity resolution process.</p>"},{"location":"21-how-it-works/#the-business-card-analogy","title":"The \"Business Card\" Analogy","text":"<p>Imagine a box of mixed-up business cards. *   Card A: \"John Smith\", <code>john@gmail.com</code> *   Card B: \"J. Smith\", <code>john@gmail.com</code> *   Card C: \"Johnny S.\", Phone <code>555-1234</code> (and <code>555-1234</code> belongs to J. Smith too)</p>"},{"location":"21-how-it-works/#step-1-matching","title":"Step 1: Matching","text":"<p>We look for shared information. *   Card A and Card B share the email <code>john@gmail.com</code>. They are stapled together.</p>"},{"location":"21-how-it-works/#step-2-chaining-label-propagation","title":"Step 2: Chaining (Label Propagation)","text":"<ul> <li>Card B also has phone <code>555-1234</code>.</li> <li>Card C has phone <code>555-1234</code>.</li> <li>So B is linked to C.</li> <li>Result: A is linked to B, and B is linked to C. Therefore, A, B, and C are all the same person.</li> </ul>"},{"location":"21-how-it-works/#step-3-golden-record","title":"Step 3: Golden Record","text":"<p>We create a new, clean card for the group. *   Name: \"John Smith\" (Best name) *   Email: \"john@gmail.com\" *   Phone: \"555-1234\"</p>"},{"location":"22-metrics/","title":"Metrics Reference","text":"<p>Metrics are available in two forms: - Warehouse-level run metrics in <code>idr_out.*</code> tables. - API operational metrics exposed in Prometheus format at <code>/metrics</code>.</p>"},{"location":"22-metrics/#key-metrics","title":"Key Metrics","text":"Metric Type Description <code>idr_run_duration_seconds</code> Gauge Total execution time <code>idr_entities_processed</code> Counter Number of source records processed <code>idr_edges_created</code> Counter Number of graph edges formed <code>idr_clusters_count</code> Gauge Total unique clusters <code>idr_lp_iterations</code> Gauge Iterations to convergence <code>idr_groups_skipped</code> Counter \"Supernodes\" skipped due to size limit"},{"location":"22-metrics/#monitoring","title":"Monitoring","text":""},{"location":"22-metrics/#prometheus-endpoint","title":"Prometheus Endpoint","text":"<p>The API exposes: - <code>GET /metrics</code> (Prometheus/OpenMetrics text payload)</p> <p>Core API metrics: - <code>idr_http_requests_total{method,path,status}</code> - <code>idr_http_request_duration_seconds{method,path}</code> - <code>idr_api_db_connected</code></p>"},{"location":"22-metrics/#alerts","title":"Alerts","text":"<ul> <li>High Skipped Groups: indicates <code>max_group_size</code> is too low or data quality issues (e.g., widespread default values).</li> <li>Max Iterations Reached: indicates graph did not converge (infinite loop or very deep chains).</li> </ul>"},{"location":"23-benchmarks/","title":"Benchmarks","text":"<p>Performance tests on standard datasets.</p>"},{"location":"23-benchmarks/#10-million-entities","title":"10 Million Entities","text":"Platform Rows Total Time Notes DuckDB 10M 143s M1 Max, Local NVMe Snowflake 10M 168s Standard Warehouse Databricks 10M 317s SQL Warehouse (Serverless) BigQuery 7M* 383s Scaled estimate"},{"location":"23-benchmarks/#throughput","title":"Throughput","text":"<ul> <li>DuckDB: ~70k entities/sec</li> <li>Snowflake: ~60k entities/sec</li> </ul>"},{"location":"23-benchmarks/#observations","title":"Observations","text":"<ul> <li>Label Propagation is the most expensive step (30-50% of time).</li> <li>Edge Building scales linearly with <code>max_group_size</code> limits.</li> </ul>"},{"location":"24-sizing/","title":"Cluster Sizing","text":"<p>Recommended resources for different data scales.</p>"},{"location":"24-sizing/#duckdb-local","title":"DuckDB (Local)","text":"Rows RAM Storage &lt; 1M 4GB 1GB 10M 16GB 5GB 50M+ 64GB 20GB"},{"location":"24-sizing/#snowflake","title":"Snowflake","text":"Rows Warehouse Size &lt; 1M X-Small 10M Medium 100M 2X-Large"},{"location":"24-sizing/#databricks","title":"Databricks","text":"Rows Cluster &lt; 10M Standard (4 workers) 100M+ Photon (16+ workers)"},{"location":"24-sizing/#spark-configs","title":"Spark Configs","text":"<p>For large jobs (&gt;10M rows): <pre><code>spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n</code></pre></p>"},{"location":"25-scaling/","title":"Scaling Considerations","text":"<p>Strategies for processing 100M+ entities.</p>"},{"location":"25-scaling/#bottlenecks","title":"Bottlenecks","text":""},{"location":"25-scaling/#1-supernodes-giant-groups","title":"1. Supernodes (Giant Groups)","text":"<p>A single identifier (e.g., <code>null</code>) connecting 1M records creates 1 trillion potential edges ($N^2$). *   Fix: Use <code>identifier_exclusion</code> table. *   Fix: Set <code>max_group_size</code> (default 50) in rules.</p>"},{"location":"25-scaling/#2-deep-chains","title":"2. Deep Chains","text":"<p>A string of A-&gt;B-&gt;C...-&gt;Z that requires many Label Propagation iterations. *   Fix: Increase <code>max_lp_iterations</code>. *   Fix: Investigate data quality (shared devices/phones).</p>"},{"location":"25-scaling/#partitioning","title":"Partitioning","text":"<p>For tables &gt; 100GB: *   Cluster <code>identifiers</code> by <code>(identifier_type, identifier_value_norm)</code>. *   Cluster <code>edges</code> by <code>identifier_type</code>.</p>"},{"location":"25-scaling/#incremental-mode","title":"Incremental Mode","text":"<p>Use <code>idr run --mode INCR</code> for daily updates. It only processes: 1.  New records. 2.  Records connected to new records. This reduces volume by 95%+.</p>"},{"location":"26-production-hardening/","title":"Production Hardening","text":"<p>Best practices for running SQL Identity Resolution in production environments.</p>"},{"location":"26-production-hardening/#data-quality-controls","title":"Data Quality Controls","text":""},{"location":"26-production-hardening/#max_group_size","title":"max_group_size","text":"<p>Prevents generic identifiers from creating mega-clusters.</p> <pre><code>-- Set appropriate limits\nUPDATE idr_meta.rule SET max_group_size = 10000 WHERE identifier_type = 'EMAIL';\nUPDATE idr_meta.rule SET max_group_size = 5000 WHERE identifier_type = 'PHONE';\nUPDATE idr_meta.rule SET max_group_size = 1 WHERE identifier_type = 'SSN';\n</code></pre> <p>What happens when exceeded: 1. Identifier group is skipped 2. Entities become singletons (resolved_id = entity_key) 3. Logged to <code>idr_out.skipped_identifier_groups</code></p>"},{"location":"26-production-hardening/#identifier-exclusions","title":"Identifier Exclusions","text":"<p>Block known bad identifiers:</p> <pre><code>-- Exact matches\nINSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', 'test@test.com', FALSE, 'Generic test'),\n  ('EMAIL', 'null@null.com', FALSE, 'Null placeholder'),\n  ('PHONE', '0000000000', FALSE, 'Invalid');\n\n-- Patterns (LIKE syntax)\nINSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', '%@example.com', TRUE, 'Example domain'),\n  ('EMAIL', 'noreply@%', TRUE, 'No-reply');\n</code></pre>"},{"location":"26-production-hardening/#large-cluster-monitoring","title":"Large Cluster Monitoring","text":""},{"location":"26-production-hardening/#alerting","title":"Alerting","text":"<p>Run warnings appear in <code>idr_out.run_history</code>:</p> <pre><code>SELECT run_id, status, large_clusters, groups_skipped, warnings\nFROM idr_out.run_history\nWHERE status = 'SUCCESS_WITH_WARNINGS'\nORDER BY started_at DESC;\n</code></pre>"},{"location":"26-production-hardening/#incremental-processing","title":"Incremental Processing","text":""},{"location":"26-production-hardening/#use-incr-mode","title":"Use INCR Mode","text":"<p>After initial FULL run, use INCR for efficiency:</p> <pre><code># First time\nidr run --platform [platform] --mode FULL\n\n# Subsequent runs\nidr run --platform [platform] --mode INCR\n</code></pre>"},{"location":"26-production-hardening/#watermark-management","title":"Watermark Management","text":"<pre><code>-- Reset watermark (force reprocess)\nUPDATE idr_meta.run_state\nSET last_watermark_value = '1900-01-01'::TIMESTAMP\nWHERE table_id = 'customers';\n</code></pre>"},{"location":"26-production-hardening/#performance-optimization","title":"Performance Optimization","text":""},{"location":"26-production-hardening/#index-source-tables","title":"Index Source Tables","text":"<pre><code>-- DuckDB\nCREATE INDEX idx_customers_updated ON customers(updated_at);\nCREATE INDEX idx_customers_email ON customers(LOWER(email));\n\n-- Snowflake (clustering)\nALTER TABLE customers CLUSTER BY (updated_at);\n\n-- BigQuery (partitioning)\nCREATE TABLE customers\nPARTITION BY DATE(updated_at)\nAS SELECT * FROM raw_customers;\n</code></pre>"},{"location":"26-production-hardening/#audit-trail","title":"Audit Trail","text":""},{"location":"26-production-hardening/#run-history","title":"Run History","text":"<p>Every run is logged:</p> <pre><code>SELECT * FROM idr_out.run_history ORDER BY started_at DESC LIMIT 20;\n</code></pre>"},{"location":"26-production-hardening/#stage-metrics","title":"Stage Metrics","text":"<pre><code>SELECT * FROM idr_out.stage_metrics WHERE run_id = 'run_xyz' ORDER BY started_at;\n</code></pre>"},{"location":"26-production-hardening/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"26-production-hardening/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a bad run needs to be rolled back:</p> <ol> <li>Stop any scheduled jobs</li> <li>Identify the bad run_id</li> <li>Reset watermarks (if needed)</li> <li>Re-run with corrected configuration</li> </ol>"},{"location":"26-production-hardening/#security-best-practices","title":"Security Best Practices","text":""},{"location":"26-production-hardening/#least-privilege","title":"Least Privilege","text":"<p>Create dedicated roles:</p> <p>Snowflake <pre><code>CREATE ROLE IDR_EXECUTOR;\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE IDR_EXECUTOR;\nGRANT SELECT ON ALL TABLES IN SCHEMA crm TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_meta TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_work TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_out TO ROLE IDR_EXECUTOR;\n</code></pre></p> <p>BigQuery <pre><code># Create service account with minimal permissions\ngcloud iam service-accounts create idr-runner\n\n# Grant BigQuery Job User + specific dataset access\nbq query --use_legacy_sql=false \\\n  \"GRANT \\`roles/bigquery.dataEditor\\` ON SCHEMA idr_out TO 'serviceAccount:idr-runner@project.iam.gserviceaccount.com'\"\n</code></pre></p>"},{"location":"26-production-hardening/#secrets-management","title":"Secrets Management","text":"<ul> <li>Never hardcode credentials in scripts</li> <li>Use environment variables or secret managers</li> <li>Rotate credentials regularly</li> </ul>"},{"location":"27-distribution/","title":"Distribution Paths","text":"<p>Use this guide to choose a reproducible install path for your environment.</p>"},{"location":"27-distribution/#decision-tree","title":"Decision Tree","text":"<ol> <li>Need only CLI/library on a developer machine?</li> <li>Use <code>pip</code> extras (<code>duckdb</code>, <code>api</code>, <code>enterprise</code>, <code>mcp</code>, or <code>all</code>).</li> <li>Need web UI + API locally with source checkout?</li> <li>Use <code>docker compose up --build</code>.</li> <li>Need reproducible deployment from prebuilt artifacts?</li> <li>Use <code>docker compose -f docker-compose.prod.yml --env-file .env up -d</code>.</li> <li>Need SSO + observability stack for enterprise evaluation?</li> <li>Use <code>docker compose -f docker-compose.enterprise.yml up -d --build</code>.</li> </ol>"},{"location":"27-distribution/#compatibility-matrix","title":"Compatibility Matrix","text":"Path Artifact Runtime Best For Python package Wheel / sdist (<code>dist/*</code>) Python 3.9-3.12 CLI automation, notebook workflows Dev Compose Source build (<code>idr_api/Dockerfile</code>, <code>idr_ui/Dockerfile</code>) Docker Engine + Compose Local development Prod Compose Pinned GHCR images (<code>idr-api</code>, <code>idr-ui</code>) Docker Engine + Compose Reproducible app deployment Enterprise Compose Source build + Keycloak + Grafana + Prometheus Docker Engine + Compose Security/ops validation"},{"location":"27-distribution/#runtime-compatibility","title":"Runtime Compatibility","text":"Component Supported Python runtime 3.9, 3.10, 3.11, 3.12 Operating systems (pip path) macOS, Linux, Windows (WSL recommended for Docker workflows) Container runtime Docker Engine with Docker Compose v2"},{"location":"27-distribution/#reproducible-commands","title":"Reproducible Commands","text":""},{"location":"27-distribution/#python-artifact-wheelsdist","title":"Python Artifact (wheel/sdist)","text":"<pre><code>python -m pip install --upgrade pip\npython -m pip install -r requirements/release.lock\npython -m build\ntwine check dist/*\npython -m venv .venv-smoke\n. .venv-smoke/bin/activate\npip install dist/*.whl\nidr version\n</code></pre>"},{"location":"27-distribution/#production-compose-pinned-images","title":"Production Compose (Pinned Images)","text":"<pre><code>cp .env.example .env\n# Set IDR_IMAGE_TAG to a released version, then:\ndocker compose -f docker-compose.prod.yml --env-file .env up -d\n</code></pre>"},{"location":"27-distribution/#local-dev-compose-build-from-source","title":"Local Dev Compose (Build From Source)","text":"<pre><code>docker compose up --build\n</code></pre>"},{"location":"27-distribution/#ci-lockfiles","title":"CI Lockfiles","text":"<p>Pinned lockfiles used in automation: - <code>requirements/ci.lock</code> - <code>requirements/release.lock</code> - <code>requirements/docs.lock</code></p> <p>UI dependency lockfile: - <code>idr_ui/package-lock.json</code> (validated in CI via <code>npm ci</code>)</p> <p>Enterprise session-store extension: - <code>IDR_SESSION_STORE_CLASS=idr_enterprise.session_store.EnterpriseInMemoryConnectionSessionStore</code> - Redis-backed option: <code>IDR_SESSION_STORE_CLASS=idr_enterprise.session_store.RedisConnectionSessionStore</code></p>"}]}